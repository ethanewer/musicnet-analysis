{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import music21 as m21\n",
    "from midiutil import MIDIFile\n",
    "import json\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp, jit, grad, Array\n",
    "from jax.typing import ArrayLike\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.core import FrozenDict\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preproscessing for Note Sequence Classification\n",
    "- Load data\n",
    "- Convert raw audio to spectrograms\n",
    "- Get notes at every timestep of each spectrogram\n",
    "- Truncate data so all sequences are of equal length\n",
    "- Split data into train and test sets\n",
    "- Load proscessed data into TensorFlow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob('../musicnet/musicnet/*/*.wav')\n",
    "label_files = glob('../musicnet/musicnet/*/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "hop_length = 512\n",
    "n_mels = 512\n",
    "\n",
    "def wav_to_mel_spec(path: str) -> np.ndarray:\n",
    "  y, _ = librosa.load(path)\n",
    "  spec = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=hop_length, n_mels=n_mels)\n",
    "  return librosa.amplitude_to_db(spec, ref=np.max).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {path[-8:-4]: wav_to_mel_spec(path) for path in data_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_note = 21\n",
    "max_note = 104\n",
    "num_notes = max_note - min_note + 1\n",
    "\n",
    "labels = {}\n",
    "\n",
    "for path in label_files:\n",
    "  key = path[-8:-4]\n",
    "  df = pd.read_csv(path)\n",
    "  label_mat = np.zeros((len(data[key]), num_notes), np.float32)\n",
    "\n",
    "  for row in df.itertuples():\n",
    "    note = row.note\n",
    "    start = row.start_time // 1024\n",
    "    end = row.end_time // 1024\n",
    "    label_mat[start:end, note - min_note] = 1\n",
    "\n",
    "  labels[key] = label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = sorted(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_len = 512\n",
    "\n",
    "truncated_data = []\n",
    "truncated_labels = []\n",
    "\n",
    "for key in keys:\n",
    "  x = data[key]\n",
    "  y = labels[key]\n",
    "  for i in range(0, x.shape[0] - truncated_len + 1, truncated_len):\n",
    "    truncated_data.append(x[i:i + truncated_len])\n",
    "    truncated_labels.append(y[i:i + truncated_len])\n",
    "\n",
    "truncated_data = np.array(truncated_data)\n",
    "truncated_labels = np.array(truncated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_test, y1_train, y1_test = train_test_split(truncated_data, truncated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds1 = tf.data.Dataset.from_tensor_slices((x1_train, y1_train)).batch(batch_size=16)\n",
    "test_ds1 = tf.data.Dataset.from_tensor_slices((x1_test, y1_test)).batch(batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model for Note Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  features: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: ArrayLike) -> Array:\n",
    "    ScanLSTM = nn.scan(\n",
    "      nn.LSTMCell, \n",
    "      variable_broadcast='params',\n",
    "      split_rngs={'params': False}, \n",
    "      in_axes=1, \n",
    "      out_axes=1,\n",
    "    )\n",
    "\n",
    "    lstm = ScanLSTM(features=128)\n",
    "    carry = lstm.initialize_carry(jax.random.key(0), x[:, 0].shape)\n",
    "    carry, x = lstm(carry, x)\n",
    "\n",
    "    x = nn.Dense(features=128)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=self.features)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainState = train_state.TrainState\n",
    "\n",
    "def create_train_state(model: LSTM, x: ArrayLike, rng_key: Array, learning_rate: float) -> TrainState:\n",
    "  params = model.init(rng_key, x=x)['params']\n",
    "  tx = optax.adam(learning_rate=learning_rate)\n",
    "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_state(model: LSTM, learning_rate: float, params_path: str) -> TrainState:\n",
    "  with open(params_path, 'r') as f:\n",
    "    params_dict = json.load(f)\n",
    "  \n",
    "  def freeze_dict(unfrozen_dict: dict[any]) -> FrozenDict[any]:\n",
    "    frozen_dict = {}\n",
    "    for k, v in unfrozen_dict.items():\n",
    "      if isinstance(v, dict):\n",
    "        frozen_dict[k] = freeze_dict(v)\n",
    "      else:\n",
    "        frozen_dict[k] = jnp.array(v)\n",
    "    return FrozenDict(frozen_dict) \n",
    "\n",
    "  params = freeze_dict(params_dict)\n",
    "  tx = optax.adam(learning_rate=learning_rate)\n",
    "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step_bce(state: TrainState, batch: tuple[ArrayLike, ArrayLike]) -> TrainState:\n",
    "\n",
    "  def loss_fn(params: FrozenDict) -> Array:\n",
    "    logits = state.apply_fn({'params': params}, x=batch[0])\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits=logits, labels=batch[1]).mean()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_metrics_bce(state: TrainState, batch: ArrayLike) -> tuple[float, float]:\n",
    "  logits = state.apply_fn({'params': state.params}, x=batch[0])\n",
    "  loss = optax.sigmoid_binary_cross_entropy(logits=logits, labels=batch[1]).mean()\n",
    "  preds = jnp.round(nn.sigmoid(logits))\n",
    "  acc = jnp.mean(preds == batch[1])\n",
    "  return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LSTM(num_notes)\n",
    "# state1 = create_train_state(model1, x1_train[:1], jax.random.PRNGKey(0), learning_rate=1e-4)\n",
    "state1 = load_train_state(model1, learning_rate=1e-4, params_path='checkpoints/lstm1-params.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model1(epoch: int) -> None:\n",
    "  train_loss_list = []\n",
    "  train_acc_list = []\n",
    "  test_loss_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  for batch in train_ds1.as_numpy_iterator():\n",
    "    loss, acc = compute_metrics_bce(state1, batch)\n",
    "    train_loss_list.append(loss)\n",
    "    train_acc_list.append(acc)\n",
    "\n",
    "  for batch in test_ds1.as_numpy_iterator():\n",
    "    loss, acc = compute_metrics_bce(state1, batch)\n",
    "    test_loss_list.append(loss)\n",
    "    test_acc_list.append(acc)\n",
    "\n",
    "  train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "  train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "  test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "  test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "  print(\n",
    "    f'[epoch {epoch}]',\n",
    "    f'train loss: {train_loss},', \n",
    "    f'train acc: {train_acc},', \n",
    "    f'test loss: {test_loss},',\n",
    "    f'test acc: {test_acc},',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] train loss: 0.1401427984237671, train acc: 0.9632152318954468, test loss: 0.13950517773628235, test acc: 0.9633268117904663,\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#   for batch in train_ds1.as_numpy_iterator():\n",
    "#     state1 = train_step_bce(state1, batch)\n",
    "  \n",
    "#   test_model1(epoch + 1)\n",
    "\n",
    "test_model1(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_dict(frozen_dict: FrozenDict[any]) -> dict[any]:\n",
    "  unfrozen_dict = {}\n",
    "  for k, v in frozen_dict.items():\n",
    "    if isinstance(v, FrozenDict) or isinstance(v, dict):\n",
    "      unfrozen_dict[k] = unfreeze_dict(v)\n",
    "    else:\n",
    "      unfrozen_dict[k] = v.tolist()\n",
    "  return unfrozen_dict\n",
    "\n",
    "# params_save_path1 = 'checkpoints/lstm1-params.json'\n",
    "# params_dict1 = unfreeze_dict(state1.params)\n",
    "# with open(params_save_path1, 'w') as f:\n",
    "#   json.dump(params_dict1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preproscessing for Note Times to Beat Regression\n",
    "- Load data\n",
    "- Convert data to intervals (if from model 1 output)\n",
    "- Truncate data\n",
    "- Load new training data and labels\n",
    "- Load data into TensorFlow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_notes(x: ArrayLike) -> list[tuple[float, float, int]]:\n",
    "  notes = []\n",
    "  for j in range(num_notes):\n",
    "    in_note = False\n",
    "    begin = 0\n",
    "    for i in range(len(x)):\n",
    "      if x[i, j] > 0:\n",
    "        if not in_note:\n",
    "          in_note = True\n",
    "          begin = i * 1024 / 44100\n",
    "      else:\n",
    "        if in_note:\n",
    "          in_note = False\n",
    "          end = i * 1024 / 44100\n",
    "          notes.append((begin, end, j + min_note))\n",
    "  \n",
    "  return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_value_set = set()\n",
    "\n",
    "for path in label_files:\n",
    "  df = pd.read_csv(path)\n",
    "  for row in df.itertuples():\n",
    "    note_value_set.add(row.note_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_values_to_nums = {v: i for i, v in enumerate(sorted(note_value_set))}\n",
    "nums_to_note_values = {v: k for k, v in note_values_to_nums.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_value_nums_to_beats = {\n",
    "  0: 0.75,\n",
    "  1: 3,\n",
    "  2: 1.5,\n",
    "  3: 0.375,\n",
    "  4: 0.5,\n",
    "  5: 2,\n",
    "  6: 1,\n",
    "  7: 0.25,\n",
    "  8: 0.0625,\n",
    "  9: 0.125,\n",
    "  10: 1.25,\n",
    "  11: 1.125,\n",
    "  12: 1 / 3,\n",
    "  13: 0.25 / 3,\n",
    "  14: 0.0625 / 3,\n",
    "  15: 0.125 / 3,\n",
    "  16: 1,\n",
    "  17: 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "note_values = []\n",
    "\n",
    "for path in label_files:\n",
    "  piece_times = []\n",
    "  piece_note_values = []\n",
    "\n",
    "  df = pd.read_csv(path)\n",
    "\n",
    "  for row in df.itertuples():\n",
    "    piece_times.append([row.start_time / 44100, row.end_time / 44100])\n",
    "    piece_note_values.append(note_values_to_nums[row.note_value])\n",
    "\n",
    "  times.append(piece_times)\n",
    "  note_values.append(piece_note_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_len = 64\n",
    "\n",
    "truncated_times = []\n",
    "truncated_note_values = []\n",
    "\n",
    "for x, y in zip(times, note_values):\n",
    "  for i in range(0, len(x) - truncated_len + 1, truncated_len):\n",
    "    truncated_times.append(x[i:i + truncated_len])\n",
    "    truncated_note_values.append(y[i:i + truncated_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_full = np.array(truncated_times, np.float32)\n",
    "y2_full = np.array(truncated_note_values, np.int32)\n",
    "\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2_full, y2_full)\n",
    "\n",
    "train_ds2 = tf.data.Dataset.from_tensor_slices((x2_train, y2_train)).batch(batch_size=16)\n",
    "test_ds2 = tf.data.Dataset.from_tensor_slices((x2_test, y2_test)).batch(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2(nn.Module):\n",
    "  features: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: ArrayLike) -> Array:\n",
    "    ScanLSTM = nn.scan(\n",
    "      nn.LSTMCell, \n",
    "      variable_broadcast='params',\n",
    "      split_rngs={'params': False}, \n",
    "      in_axes=1, \n",
    "      out_axes=1,\n",
    "    )\n",
    "\n",
    "    lstm1 = ScanLSTM(features=128)\n",
    "    carry = lstm1.initialize_carry(jax.random.key(0), x[:, 0].shape)\n",
    "    carry, x = lstm1(carry, x)\n",
    "\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=128)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=64)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=32)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=self.features)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step_cce(state: TrainState, batch: tuple[ArrayLike, ArrayLike]) -> TrainState:\n",
    "\n",
    "  def loss_fn(params: FrozenDict) -> Array:\n",
    "    logits = state.apply_fn({'params': params}, x=batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_metrics_cce(state: TrainState, batch: ArrayLike) -> tuple[float, float]:\n",
    "  logits = state.apply_fn({'params': state.params}, x=batch[0])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "  preds = jnp.argmax(logits, axis=2)\n",
    "  acc = jnp.mean(preds == batch[1])\n",
    "  return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LSTM2(18)\n",
    "state2 = create_train_state(model2, x2_train[:1], jax.random.PRNGKey(0), learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model2(epoch: int) -> None:\n",
    "  train_loss_list = []\n",
    "  train_acc_list = []\n",
    "  test_loss_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  for batch in train_ds2.as_numpy_iterator():\n",
    "    loss, acc = compute_metrics_cce(state2, batch)\n",
    "    train_loss_list.append(loss)\n",
    "    train_acc_list.append(acc)\n",
    "\n",
    "  for batch in test_ds2.as_numpy_iterator():\n",
    "    loss, acc = compute_metrics_cce(state2, batch)\n",
    "    test_loss_list.append(loss)\n",
    "    test_acc_list.append(acc)\n",
    "\n",
    "  train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "  train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "  test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "  test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "  print(\n",
    "    f'[epoch {epoch}]',\n",
    "    f'train loss: {train_loss},', \n",
    "    f'train acc: {train_acc},', \n",
    "    f'test loss: {test_loss},',\n",
    "    f'test acc: {test_acc},',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] train loss: 1.8079787492752075, train acc: 0.3520481288433075, test loss: 1.809807300567627, test acc: 0.353514164686203,\n",
      "[epoch 20] train loss: 1.778620719909668, train acc: 0.3688410818576813, test loss: 1.7797287702560425, test acc: 0.3674545884132385,\n",
      "[epoch 30] train loss: 1.7525087594985962, train acc: 0.379303514957428, test loss: 1.754604697227478, test acc: 0.37645891308784485,\n",
      "[epoch 40] train loss: 1.742892861366272, train acc: 0.3841315507888794, test loss: 1.7450793981552124, test acc: 0.3815096616744995,\n",
      "[epoch 50] train loss: 1.730048418045044, train acc: 0.3914686441421509, test loss: 1.7320880889892578, test acc: 0.3890003561973572,\n",
      "[epoch 60] train loss: 1.7169861793518066, train acc: 0.39790603518486023, test loss: 1.7196967601776123, test acc: 0.39521780610084534,\n",
      "[epoch 70] train loss: 1.7131528854370117, train acc: 0.4000147581100464, test loss: 1.7159243822097778, test acc: 0.3974764943122864,\n",
      "[epoch 80] train loss: 1.7092208862304688, train acc: 0.40230268239974976, test loss: 1.7140110731124878, test acc: 0.39968040585517883,\n",
      "[epoch 90] train loss: 1.6989374160766602, train acc: 0.40513041615486145, test loss: 1.7059372663497925, test acc: 0.4018946886062622,\n",
      "[epoch 100] train loss: 1.6996033191680908, train acc: 0.4050103425979614, test loss: 1.7074581384658813, test acc: 0.40174227952957153,\n",
      "[epoch 110] train loss: 1.688992977142334, train acc: 0.4081506133079529, test loss: 1.6998854875564575, test acc: 0.4035097062587738,\n",
      "[epoch 120] train loss: 1.6806226968765259, train acc: 0.4109817445278168, test loss: 1.6941568851470947, test acc: 0.4045521020889282,\n",
      "[epoch 130] train loss: 1.673427939414978, train acc: 0.41435545682907104, test loss: 1.6877386569976807, test acc: 0.4073353111743927,\n",
      "[epoch 140] train loss: 1.6730995178222656, train acc: 0.41381052136421204, test loss: 1.6899824142456055, test acc: 0.4061923027038574,\n",
      "[epoch 150] train loss: 1.671654224395752, train acc: 0.4144718647003174, test loss: 1.6914865970611572, test acc: 0.4058060944080353,\n",
      "[epoch 160] train loss: 1.6721043586730957, train acc: 0.4144670069217682, test loss: 1.6935968399047852, test acc: 0.4053437113761902,\n",
      "[epoch 170] train loss: 1.6703996658325195, train acc: 0.4150736629962921, test loss: 1.6938186883926392, test acc: 0.40537330508232117,\n",
      "[epoch 180] train loss: 1.6681146621704102, train acc: 0.4154927432537079, test loss: 1.6954752206802368, test acc: 0.4054376780986786,\n",
      "[epoch 190] train loss: 1.6678099632263184, train acc: 0.41519299149513245, test loss: 1.696567416191101, test acc: 0.40446409583091736,\n",
      "[epoch 200] train loss: 1.6678434610366821, train acc: 0.4148258566856384, test loss: 1.6997147798538208, test acc: 0.40344682335853577,\n",
      "[epoch 210] train loss: 1.6661717891693115, train acc: 0.41522401571273804, test loss: 1.699607014656067, test acc: 0.4039173424243927,\n",
      "[epoch 220] train loss: 1.6690729856491089, train acc: 0.4139283299446106, test loss: 1.7042293548583984, test acc: 0.4026840627193451,\n",
      "[epoch 230] train loss: 1.6648811101913452, train acc: 0.4146351218223572, test loss: 1.7014280557632446, test acc: 0.403899610042572,\n",
      "[epoch 240] train loss: 1.666070580482483, train acc: 0.4149330258369446, test loss: 1.7072234153747559, test acc: 0.4039299190044403,\n",
      "[epoch 250] train loss: 1.6595921516418457, train acc: 0.41641557216644287, test loss: 1.7002619504928589, test acc: 0.405548632144928,\n",
      "[epoch 260] train loss: 1.6697007417678833, train acc: 0.41365665197372437, test loss: 1.7138333320617676, test acc: 0.4030584394931793,\n",
      "[epoch 270] train loss: 1.6619012355804443, train acc: 0.41606611013412476, test loss: 1.7086491584777832, test acc: 0.40427541732788086,\n",
      "[epoch 280] train loss: 1.6659440994262695, train acc: 0.4175187051296234, test loss: 1.7158490419387817, test acc: 0.4050699770450592,\n",
      "[epoch 290] train loss: 1.6613811254501343, train acc: 0.4180808663368225, test loss: 1.7135981321334839, test acc: 0.4054768979549408,\n",
      "[epoch 300] train loss: 1.6536165475845337, train acc: 0.41991284489631653, test loss: 1.7077606916427612, test acc: 0.4078398644924164,\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_ds2.as_numpy_iterator():\n",
    "    state2 = train_step_cce(state2, batch)\n",
    "  \n",
    "  if (epoch + 1) % 10 == 0:\n",
    "    test_model2(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_save_path2 = 'checkpoints/lstm2-params.json'\n",
    "params_dict2 = unfreeze_dict(state2.params)\n",
    "with open(params_save_path2, 'w') as f:\n",
    "  json.dump(params_dict2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi_file(notes, output_file='output.mid', tempo=120):\n",
    "  midi = MIDIFile(1)\n",
    "  midi.addTempo(0, 0, tempo)\n",
    "  \n",
    "  for start, duration, note_num in notes:\n",
    "    track = 0\n",
    "    channel = 0  \n",
    "    volume = 100\n",
    "\n",
    "    midi.addNote(track, channel, note_num, start, duration, volume)\n",
    "\n",
    "  with open(output_file, 'wb') as midi_file:\n",
    "    midi.writeFile(midi_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_midi_file(file_path='output.mid'):\n",
    "  mf = m21.midi.MidiFile()\n",
    "  mf.open(file_path)\n",
    "  mf.read()\n",
    "  mf.close()\n",
    "\n",
    "  score = m21.midi.translate.midiFileToStream(mf)\n",
    "  \n",
    "  print(f'Duration: {score.highestTime} seconds')\n",
    "\n",
    "  sp = m21.midi.realtime.StreamPlayer(score)\n",
    "  sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_notes = matrix_to_notes(labels['2242'])\n",
    "x_times = np.array([[[s, e] for s, e, _ in x_notes[:truncated_len]]], np.float32) / 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 2)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_beats = state2.apply_fn({'params': state2.params}, x=x_times)\n",
    "x_beats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_notes = []\n",
    "for i in range(truncated_len):\n",
    "  new_x_notes.append((float(x_beats[0, i, 0]), float(x_beats[0, i, 1]), x_notes[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_notes = []\n",
    "\n",
    "df = pd.read_csv('../musicnet/musicnet/train_labels/2242.csv')\n",
    "\n",
    "for row in df.itertuples():\n",
    "  real_notes.append((row.start_beat, row.end_beat, row.note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'550'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_midi_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_x_notes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[123], line 13\u001b[0m, in \u001b[0;36mcreate_midi_file\u001b[0;34m(notes, output_file, tempo)\u001b[0m\n\u001b[1;32m     10\u001b[0m   midi\u001b[38;5;241m.\u001b[39maddNote(track, channel, note_num, start, duration, volume)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m midi_file:\n\u001b[0;32m---> 13\u001b[0m   \u001b[43mmidi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmidi_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default/lib/python3.11/site-packages/midiutil/MidiFile.py:1637\u001b[0m, in \u001b[0;36mMIDIFile.writeFile\u001b[0;34m(self, fileHandle)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mwriteFile(fileHandle)\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;66;03m# Close the tracks and have them create the MIDI event data structures.\u001b[39;00m\n\u001b[0;32m-> 1637\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;66;03m# Write the MIDI Events to file.\u001b[39;00m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumTracks):\n",
      "File \u001b[0;32m~/default/lib/python3.11/site-packages/midiutil/MidiFile.py:1688\u001b[0m, in \u001b[0;36mMIDIFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumTracks):\n\u001b[0;32m-> 1688\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcloseTrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1689\u001b[0m     \u001b[38;5;66;03m# We want things like program changes to come before notes when\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;66;03m# they are at the same time, so we sort the MIDI events by both\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;66;03m# their start time and a secondary ordinality defined for each kind\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;66;03m# of event.\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks[i]\u001b[38;5;241m.\u001b[39mMIDIEventList\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39msort_events)\n",
      "File \u001b[0;32m~/default/lib/python3.11/site-packages/midiutil/MidiFile.py:826\u001b[0m, in \u001b[0;36mMIDITrack.closeTrack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremdep:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremoveDuplicates()\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessEventList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default/lib/python3.11/site-packages/midiutil/MidiFile.py:789\u001b[0m, in \u001b[0;36mMIDITrack.processEventList\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIDIEventList\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39msort_events)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeinterleave:\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeInterleaveNotes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default/lib/python3.11/site-packages/midiutil/MidiFile.py:885\u001b[0m, in \u001b[0;36mMIDITrack.deInterleaveNotes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m     tempEventList\u001b[38;5;241m.\u001b[39mappend(event)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mevtname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoteOff\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mstack\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoteeventkey\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    886\u001b[0m         event\u001b[38;5;241m.\u001b[39mtick \u001b[38;5;241m=\u001b[39m stack[noteeventkey]\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    887\u001b[0m         tempEventList\u001b[38;5;241m.\u001b[39mappend(event)\n",
      "\u001b[0;31mKeyError\u001b[0m: '550'"
     ]
    }
   ],
   "source": [
    "create_midi_file(new_x_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2544, 512, 512), (2544, 512, 84))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_test.shape, y1_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 256, 2), (1022, 256))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_test.shape, y2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
