{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp, jit, grad, Array\n",
    "from jax.typing import ArrayLike\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.core import FrozenDict\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preproscessing\n",
    "- Load data\n",
    "- Convert raw audio to spectrograms\n",
    "- Get notes at every timestep of each spectrogram\n",
    "- Truncate data so all sequences are of equal length\n",
    "- Split data into train and test sets\n",
    "- Load proscessed data into TensorFlow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob('../musicnet/musicnet/*/*.wav')\n",
    "label_files = glob('../musicnet/musicnet/*/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "hop_length = 512\n",
    "n_mels = 512\n",
    "\n",
    "def wav_to_mel_spec(path: str) -> np.ndarray:\n",
    "  y, _ = librosa.load(path)\n",
    "  spec = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=hop_length, n_mels=n_mels)\n",
    "  return librosa.amplitude_to_db(spec, ref=np.max).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {path[-8:-4]: wav_to_mel_spec(path) for path in data_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_note = 21\n",
    "max_note = 104\n",
    "num_notes = max_note - min_note + 1\n",
    "\n",
    "labels = {}\n",
    "\n",
    "for path in label_files:\n",
    "  key = path[-8:-4]\n",
    "  df = pd.read_csv(path)\n",
    "  label_mat = np.zeros((len(data[key]), num_notes), np.float32)\n",
    "\n",
    "  for row in df.itertuples():\n",
    "    note = row.note\n",
    "    start = row.start_time // 1024\n",
    "    end = row.end_time // 1024\n",
    "    label_mat[start:end, note - min_note] = 1\n",
    "\n",
    "  labels[key] = label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = sorted(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_len = 512\n",
    "\n",
    "truncated_data = []\n",
    "truncated_labels = []\n",
    "\n",
    "for key in keys:\n",
    "  x = data[key]\n",
    "  y = labels[key]\n",
    "  for i in range(0, x.shape[0] - truncated_len + 1, truncated_len):\n",
    "    truncated_data.append(x[i:i + truncated_len])\n",
    "    truncated_labels.append(y[i:i + truncated_len])\n",
    "\n",
    "truncated_data = np.array(truncated_data)\n",
    "truncated_labels = np.array(truncated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_test, y1_train, y1_test = train_test_split(truncated_data, truncated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds1 = tf.data.Dataset.from_tensor_slices((x1_train, y1_train)).batch(batch_size=16)\n",
    "test_ds1 = tf.data.Dataset.from_tensor_slices((x1_test, y1_test)).batch(batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  features: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: ArrayLike) -> Array:\n",
    "    ScanLSTM = nn.scan(\n",
    "      nn.LSTMCell, \n",
    "      variable_broadcast='params',\n",
    "      split_rngs={'params': False}, \n",
    "      in_axes=1, \n",
    "      out_axes=1,\n",
    "    )\n",
    "\n",
    "    lstm = ScanLSTM(features=128)\n",
    "    carry = lstm.initialize_carry(jax.random.key(0), x[:, 0].shape)\n",
    "    carry, x = lstm(carry, x)\n",
    "\n",
    "    x = nn.Dense(features=128)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=self.features)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainState = train_state.TrainState\n",
    "\n",
    "def create_train_state(model: LSTM, x: ArrayLike, rng_key: Array, learning_rate: float) -> TrainState:\n",
    "  params = model.init(rng_key, x=x)['params']\n",
    "  tx = optax.adam(learning_rate=learning_rate)\n",
    "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step_bce(state: TrainState, batch: tuple[ArrayLike, ArrayLike]) -> TrainState:\n",
    "\n",
    "  def loss_fn(params: FrozenDict) -> Array:\n",
    "    logits = state.apply_fn({'params': params}, x=batch[0])\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits=logits, labels=batch[1]).mean()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_metrics_bce(state: TrainState, batch: ArrayLike) -> tuple[float, float]:\n",
    "  logits = state.apply_fn({'params': state.params}, x=batch[0])\n",
    "  loss = optax.sigmoid_binary_cross_entropy(logits=logits, labels=batch[1]).mean()\n",
    "  preds = jnp.round(nn.sigmoid(logits))\n",
    "  acc = jnp.mean(preds == batch[1])\n",
    "  return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LSTM(num_notes)\n",
    "state1 = create_train_state(model1, x1_train[:1], jax.random.PRNGKey(0), learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for batch in train_ds1.as_numpy_iterator():\n",
    "  loss, acc = compute_metrics_bce(state1, batch)\n",
    "  train_loss_list.append(loss)\n",
    "  train_acc_list.append(acc)\n",
    "\n",
    "for batch in test_ds1.as_numpy_iterator():\n",
    "  loss, acc = compute_metrics_bce(state1, batch)\n",
    "  test_loss_list.append(loss)\n",
    "  test_acc_list.append(acc)\n",
    "\n",
    "train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "print(\n",
    "  f'train loss: {train_loss},', \n",
    "  f'train acc: {train_acc},', \n",
    "  f'test loss: {test_loss},',\n",
    "  f'test acc: {test_acc},',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_ds1.as_numpy_iterator():\n",
    "    state1 = train_step_bce(state1, batch)\n",
    "  \n",
    "  train_loss_list = []\n",
    "  train_acc_list = []\n",
    "  test_loss_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  for batch in train_ds1.as_numpy_iterator():\n",
    "    loss, acc = compute_metrics_bce(state1, batch)\n",
    "    train_loss_list.append(loss)\n",
    "    train_acc_list.append(acc)\n",
    "\n",
    "  for batch in test_ds1.as_numpy_iterator():\n",
    "    loss, acc = compute_metrics_bce(state1, batch)\n",
    "    test_loss_list.append(loss)\n",
    "    test_acc_list.append(acc)\n",
    "  \n",
    "  train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "  train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "  test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "  test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "  print(\n",
    "    f'[epoch {epoch + 1}]', \n",
    "    f'train loss: {train_loss},', \n",
    "    f'train acc: {train_acc},', \n",
    "    f'test loss: {test_loss},',\n",
    "    f'test acc: {test_acc},',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_dict(frozen_dict: FrozenDict[any]) -> dict[any]:\n",
    "  unfrozen_dict = {}\n",
    "  for k, v in frozen_dict.items():\n",
    "    if isinstance(v, FrozenDict) or isinstance(v, dict):\n",
    "      unfrozen_dict[k] = unfreeze_dict(v)\n",
    "    else:\n",
    "      unfrozen_dict[k] = v.tolist()\n",
    "  return unfrozen_dict\n",
    "\n",
    "params_save_path1 = 'checkpoints/lstm1-params.json'\n",
    "params_dict1 = unfreeze_dict(state1.params)\n",
    "with open(params_save_path1, 'w') as f:\n",
    "  json.dump(params_dict1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_notes(x: ArrayLike) -> list[tuple[float, float, int]]:\n",
    "  notes = []\n",
    "  for j in range(num_notes):\n",
    "    in_note = False\n",
    "    begin = 0\n",
    "    for i in range(len(x)):\n",
    "      if x[i, j] > 0:\n",
    "        if not in_note:\n",
    "          in_note = True\n",
    "          begin = i * 1024\n",
    "      else:\n",
    "        if in_note:\n",
    "          in_note = False\n",
    "          end = i * 1024\n",
    "          notes.append((begin, end, j + min_note))\n",
    "  \n",
    "  return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = matrix_to_notes(labels['1727'])\n",
    "notes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "beats = []\n",
    "\n",
    "for path in label_files:\n",
    "  piece_times = []\n",
    "  piece_beats = []\n",
    "\n",
    "  for row in df.itertuples():\n",
    "    piece_times.append((row.start_time, row.end_time))\n",
    "    piece_beats.append((row.start_beat, row.end_beat))\n",
    "\n",
    "  times.append(piece_times)\n",
    "  beats.append(piece_beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_full = np.array(times, np.float32)\n",
    "y2_full = np.array(beats, np.float32)\n",
    "\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2_full, y2_full)\n",
    "\n",
    "train_ds2 = tf.data.Dataset.from_tensor_slices((x2_train, y2_train)).batch(batch_size=16)\n",
    "test_ds2 = tf.data.Dataset.from_tensor_slices((x2_test, y2_test)).batch(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step_mse(state: TrainState, batch: tuple[ArrayLike, ArrayLike]) -> TrainState:\n",
    "\n",
    "  def loss_fn(params: FrozenDict) -> Array:\n",
    "    logits = state.apply_fn({'params': params}, x=batch[0])\n",
    "    loss = optax.squared_error(predictions=logits, targets=batch[1]).mean()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_metrics_mse(state: TrainState, batch: ArrayLike) -> tuple[float, float]:\n",
    "  logits = state.apply_fn({'params': state.params}, x=batch[0])\n",
    "  loss = optax.squared_error(predictions=logits, targets=batch[1]).mean()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LSTM(2)\n",
    "state2 = create_train_state(model2, x2_train[:1], jax.random.PRNGKey(0), learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_ds2.as_numpy_iterator():\n",
    "    state2 = train_step_mse(state2, batch)\n",
    "  \n",
    "  if (epoch + 1) % 10 == 0:\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    for batch in train_ds2.as_numpy_iterator():\n",
    "      loss = compute_metrics_mse(state2, batch)\n",
    "      train_loss_list.append(loss)\n",
    "\n",
    "    for batch in test_ds2.as_numpy_iterator():\n",
    "      loss = compute_metrics_mse(state2, batch)\n",
    "      test_loss_list.append(loss)\n",
    "    \n",
    "    train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "    test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "\n",
    "    print(\n",
    "      f'[epoch {epoch + 1}]', \n",
    "      f'train loss: {train_loss},', \n",
    "      f'test loss: {test_loss},',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_save_path2 = 'checkpoints/lstm2-params.json'\n",
    "params_dict2 = unfreeze_dict(state2.params)\n",
    "with open(params_save_path2, 'w') as f:\n",
    "  json.dump(params_dict2, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
