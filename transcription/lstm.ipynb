{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp, jit, grad, Array\n",
    "from jax.typing import ArrayLike\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.core import FrozenDict\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preproscessing\n",
    "- Load data\n",
    "- Convert raw audio to spectrograms\n",
    "- Get notes at every timestep of each spectrogram\n",
    "- Truncate data so all sequences are of equal length\n",
    "- Split data into train and test sets\n",
    "- Load proscessed data into TensorFlow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob('../musicnet/musicnet/*/*.wav')\n",
    "label_files = glob('../musicnet/musicnet/*/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "hop_length = 512\n",
    "n_mels = 512\n",
    "\n",
    "def wav_to_mel_spec(path: str) -> np.ndarray:\n",
    "  y, _ = librosa.load(path)\n",
    "  spec = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=hop_length, n_mels=n_mels)\n",
    "  return librosa.amplitude_to_db(spec, ref=np.max).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {path[-8:-4]: wav_to_mel_spec(path) for path in data_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_note = 21\n",
    "max_note = 104\n",
    "num_notes = max_note - min_note + 1\n",
    "\n",
    "labels = {}\n",
    "\n",
    "for path in label_files:\n",
    "  key = path[-8:-4]\n",
    "  df = pd.read_csv(path)\n",
    "  label_mat = np.zeros((len(data[key]), num_notes), np.float32)\n",
    "\n",
    "  for row in df.itertuples():\n",
    "    note = row.note\n",
    "    start = row.start_time // 1024\n",
    "    end = row.end_time // 1024\n",
    "    label_mat[start:end, note - min_note] = 1\n",
    "\n",
    "  labels[key] = label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = sorted(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_len = 512\n",
    "\n",
    "truncated_data = []\n",
    "truncated_labels = []\n",
    "\n",
    "for key in keys:\n",
    "  x = data[key]\n",
    "  y = labels[key]\n",
    "  for i in range(0, x.shape[0] - truncated_len + 1, truncated_len):\n",
    "    truncated_data.append(x[i:i + truncated_len])\n",
    "    truncated_labels.append(y[i:i + truncated_len])\n",
    "\n",
    "truncated_data = np.array(truncated_data)\n",
    "truncated_labels = np.array(truncated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(truncated_data, truncated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size=batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  features: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    ScanLSTM = nn.scan(\n",
    "      nn.LSTMCell, \n",
    "      variable_broadcast='params',\n",
    "      split_rngs={'params': False}, \n",
    "      in_axes=1, \n",
    "      out_axes=1,\n",
    "    )\n",
    "\n",
    "    lstm = ScanLSTM(features=128)\n",
    "    carry = lstm.initialize_carry(jax.random.key(0), x[:, 0].shape)\n",
    "    carry, x = lstm(carry, x)\n",
    "\n",
    "    x = nn.Dense(features=128)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=self.features)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainState = train_state.TrainState\n",
    "\n",
    "def create_train_state(model: LSTM, rng_key: Array, learning_rate: float) -> TrainState:\n",
    "  params = model.init(rng_key, x=x_train[:1])['params']\n",
    "  tx = optax.adam(learning_rate=learning_rate)\n",
    "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step(state: TrainState, batch: tuple[ArrayLike, ArrayLike]) -> TrainState:\n",
    "\n",
    "  def loss_fn(params: FrozenDict) -> Array:\n",
    "    logits = state.apply_fn({'params': params}, x=batch[0])\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits=logits, labels=batch[1]).mean()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_metrics(state: TrainState, batch: ArrayLike) -> tuple[float, float]:\n",
    "  logits = state.apply_fn({'params': state.params}, x=batch[0])\n",
    "  loss = optax.sigmoid_binary_cross_entropy(logits=logits, labels=batch[1]).mean()\n",
    "  preds = jnp.round(nn.sigmoid(logits))\n",
    "  acc = jnp.mean(preds == batch[1])\n",
    "  return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(num_notes)\n",
    "state = create_train_state(model, jax.random.PRNGKey(0), learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.7163073420524597, train acc: 0.49891921877861023, test loss: 0.7164062857627869, test acc: 0.4987044334411621,\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for batch in train_ds.as_numpy_iterator():\n",
    "  loss, acc = compute_metrics(state, batch)\n",
    "  train_loss_list.append(loss)\n",
    "  train_acc_list.append(acc)\n",
    "\n",
    "for batch in test_ds.as_numpy_iterator():\n",
    "  loss, acc = compute_metrics(state, batch)\n",
    "  test_loss_list.append(loss)\n",
    "  test_acc_list.append(acc)\n",
    "\n",
    "train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "print(\n",
    "  f'train loss: {train_loss},', \n",
    "  f'train acc: {train_acc},', \n",
    "  f'test loss: {test_loss},',\n",
    "  f'test acc: {test_acc},',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train loss: 0.14465948939323425, train acc: 0.9632760286331177, test loss: 0.14487622678279877, test acc: 0.9631381034851074,\n",
      "[epoch 2] train loss: 0.1396123170852661, train acc: 0.9632760286331177, test loss: 0.1398719996213913, test acc: 0.9631381034851074,\n",
      "[epoch 3] train loss: 0.1391063928604126, train acc: 0.9632760286331177, test loss: 0.1393844038248062, test acc: 0.9631381034851074,\n",
      "[epoch 4] train loss: 0.13879244029521942, train acc: 0.9632760286331177, test loss: 0.13907095789909363, test acc: 0.9631381034851074,\n",
      "[epoch 5] train loss: 0.13887061178684235, train acc: 0.9632760286331177, test loss: 0.13914521038532257, test acc: 0.9631381034851074,\n",
      "[epoch 6] train loss: 0.13883280754089355, train acc: 0.9632760286331177, test loss: 0.1391068994998932, test acc: 0.9631381034851074,\n",
      "[epoch 7] train loss: 0.13881346583366394, train acc: 0.9632760286331177, test loss: 0.13908827304840088, test acc: 0.9631381034851074,\n",
      "[epoch 8] train loss: 0.1387949436903, train acc: 0.9632760286331177, test loss: 0.1390703022480011, test acc: 0.9631381034851074,\n",
      "[epoch 9] train loss: 0.1387903392314911, train acc: 0.9632760286331177, test loss: 0.1390661746263504, test acc: 0.9631381034851074,\n",
      "[epoch 10] train loss: 0.13878144323825836, train acc: 0.9632760286331177, test loss: 0.13905730843544006, test acc: 0.9631381034851074,\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    state = train_step(state, batch)\n",
    "  \n",
    "  train_loss_list = []\n",
    "  train_acc_list = []\n",
    "  test_loss_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    loss, acc = compute_metrics(state, batch)\n",
    "    train_loss_list.append(loss)\n",
    "    train_acc_list.append(acc)\n",
    "\n",
    "  for batch in test_ds.as_numpy_iterator():\n",
    "    loss, acc = compute_metrics(state, batch)\n",
    "    test_loss_list.append(loss)\n",
    "    test_acc_list.append(acc)\n",
    "  \n",
    "  train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "  train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "  test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "  test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "  print(\n",
    "    f'[epoch {epoch + 1}]', \n",
    "    f'train loss: {train_loss},', \n",
    "    f'train acc: {train_acc},', \n",
    "    f'test loss: {test_loss},',\n",
    "    f'test acc: {test_acc},',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_dict(frozen_dict: FrozenDict[any]) -> dict[any]:\n",
    "  unfrozen_dict = {}\n",
    "  for k, v in frozen_dict.items():\n",
    "    if isinstance(v, FrozenDict) or isinstance(v, dict):\n",
    "      unfrozen_dict[k] = unfreeze_dict(v)\n",
    "    else:\n",
    "      unfrozen_dict[k] = v.tolist()\n",
    "  return unfrozen_dict\n",
    "\n",
    "params_save_path = 'checkpoints/lstm-params.json'\n",
    "params_dict = unfreeze_dict(state.params)\n",
    "with open(params_save_path, 'w') as f:\n",
    "  json.dump(params_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
