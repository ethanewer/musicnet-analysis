{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp, jit, value_and_grad\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preproscessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('musicnet_metadata.csv')\n",
    "data_files = glob('musicnet/musicnet/*/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 512\n",
    "\n",
    "def wav_to_mel_spec(path):\n",
    "  y, sr = librosa.load(path)\n",
    "  spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "  return np.abs(librosa.amplitude_to_db(spec, ref=np.max))\n",
    "\n",
    "data = [wav_to_mel_spec(path) for path in data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = [int(path[-8:-4]) for path in data_files]\n",
    "labels = [metadata[metadata['id'] == i]['ensemble'].values[0] for i in data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_nums = {label: i for i, label in enumerate(set(labels))}\n",
    "nums_to_labels = {i: label for label, i in labels_to_nums.items()}\n",
    "\n",
    "labels = [labels_to_nums[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "new_labels = []\n",
    "\n",
    "sample_len = 512\n",
    "\n",
    "for x, y in zip(data, labels):\n",
    "  for i in range(0, x.shape[1] - sample_len + 1, sample_len):\n",
    "    new_data.append(np.expand_dims(x[:, i:i + sample_len], axis=2))\n",
    "    new_labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.array(new_data, np.float32)\n",
    "y_full = np.array(new_labels, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_full, y_full, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "batch_size = 8\n",
    "train_ds = train_ds.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, x, train):\n",
    "    x = (nn.Conv(features=16, kernel_size=(3, 3), use_bias=False))(x)\n",
    "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = nn.Conv(features=16, kernel_size=(3, 3), use_bias=False)(x)\n",
    "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = nn.Conv(features=16, kernel_size=(3, 3), use_bias=False)(x)\n",
    "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    \n",
    "    x = nn.Dense(features=128)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=21)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "  batch_stats: any\n",
    "\n",
    "def create_train_state(model, rng, learning_rate):\n",
    "  variables = model.init(rng, jnp.ones((1, *x_train.shape[1:])), train=False)\n",
    "  return TrainState.create(\n",
    "    apply_fn=model.apply, \n",
    "    params=variables['params'],\n",
    "    batch_stats=variables['batch_stats'], \n",
    "    tx=optax.adamw(learning_rate, weight_decay=1e-3),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step(state, batch):\n",
    "  def loss_fn(params):\n",
    "    logits, updates = state.apply_fn(\n",
    "      {'params': params, 'batch_stats': state.batch_stats}, \n",
    "      batch[0], \n",
    "      train=True,\n",
    "      mutable=['batch_stats'],\n",
    "    )\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, updates\n",
    "  \n",
    "  grad_fn = value_and_grad(loss_fn, has_aux=True)\n",
    "  (_, updates), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  state = state.replace(batch_stats=updates['batch_stats'])\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def accuracy(state, batch):\n",
    "  logits = state.apply_fn(\n",
    "    {'params': state.params, 'batch_stats': state.batch_stats}, \n",
    "    batch[0],\n",
    "    train=False,\n",
    "  )\n",
    "  preds = jnp.argmax(logits, axis=1)\n",
    "  acc = jnp.mean(preds == batch[1])\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_model = CNN()\n",
    "state = create_train_state(flax_model, jax.random.PRNGKey(0), learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train acc: 0.7603248953819275, test acc: 0.7114779949188232\n",
      "[epoch 2] train acc: 0.8197588920593262, test acc: 0.7405660152435303\n",
      "[epoch 3] train acc: 0.9384171962738037, test acc: 0.8278301954269409\n",
      "[epoch 4] train acc: 0.927410900592804, test acc: 0.7995283007621765\n",
      "[epoch 5] train acc: 0.9716981053352356, test acc: 0.8356918096542358\n",
      "[epoch 6] train acc: 0.9813941121101379, test acc: 0.8364779949188232\n",
      "[epoch 7] train acc: 0.990435004234314, test acc: 0.8561320900917053\n",
      "[epoch 8] train acc: 0.9990828037261963, test acc: 0.875\n",
      "[epoch 9] train acc: 0.99947589635849, test acc: 0.8730345964431763\n",
      "[epoch 10] train acc: 0.9728773832321167, test acc: 0.8297955989837646\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    state = train_step(state, batch)\n",
    "  \n",
    "  train_acc_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    acc = accuracy(state, batch)\n",
    "    train_acc_list.append(acc)\n",
    "\n",
    "  for batch in test_ds.as_numpy_iterator():\n",
    "    acc = accuracy(state, batch)\n",
    "    test_acc_list.append(acc)\n",
    "  \n",
    "  train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "  test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "  print(f'[epoch {epoch + 1}] train acc: {train_acc}, test acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/path/to/save/model'\n",
    "flax.serialization.save_checkpoint(save_path, {'params': state.params, 'batch_stats': state.batch_stats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
