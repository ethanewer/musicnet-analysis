{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp, jit, value_and_grad\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preproscessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('musicnet/musicnet_metadata.csv')\n",
    "data_files = glob('musicnet/musicnet/*/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 512\n",
    "\n",
    "def wav_to_mel_spec(path):\n",
    "  y, sr = librosa.load(path)\n",
    "  spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "  return np.abs(librosa.amplitude_to_db(spec, ref=np.max))\n",
    "\n",
    "data = [wav_to_mel_spec(path) for path in data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = [int(path[-8:-4]) for path in data_files]\n",
    "labels = [metadata[metadata['id'] == i]['ensemble'].values[0] for i in data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_nums = {label: i for i, label in enumerate(sorted(set(labels)))}\n",
    "nums_to_labels = {i: label for label, i in labels_to_nums.items()}\n",
    "\n",
    "labels = [labels_to_nums[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "new_labels = []\n",
    "\n",
    "sample_len = 512\n",
    "\n",
    "for x, y in zip(data, labels):\n",
    "  for i in range(0, x.shape[1] - sample_len + 1, sample_len):\n",
    "    new_data.append(np.expand_dims(x[:, i:i + sample_len], axis=2))\n",
    "    new_labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.array(new_data, np.float32)\n",
    "y_full = np.array(new_labels, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_full, y_full, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "batch_size = 16\n",
    "train_ds = train_ds.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, x, train):\n",
    "    x = (nn.Conv(features=8, kernel_size=(3, 3), use_bias=False))(x)\n",
    "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = nn.Conv(features=8, kernel_size=(3, 3), use_bias=False)(x)\n",
    "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = nn.Conv(features=8, kernel_size=(3, 3), use_bias=False)(x)\n",
    "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    \n",
    "    x = nn.Dense(features=128)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=21)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "  batch_stats: any\n",
    "\n",
    "def create_train_state(model, rng, learning_rate):\n",
    "  variables = model.init(rng, x=jnp.ones((1, *x_train.shape[1:])), train=False)\n",
    "  return TrainState.create(\n",
    "    apply_fn=model.apply, \n",
    "    params=variables['params'],\n",
    "    batch_stats=variables['batch_stats'], \n",
    "    tx=optax.adamw(learning_rate, weight_decay=1e-3),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step(state, batch):\n",
    "  def loss_fn(params):\n",
    "    logits, updates = state.apply_fn({\n",
    "      'params': params, \n",
    "      'batch_stats': state.batch_stats,\n",
    "    }, x=batch[0], train=True, mutable=['batch_stats'])\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, updates\n",
    "  \n",
    "  grad_fn = value_and_grad(loss_fn, has_aux=True)\n",
    "  (_, updates), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  state = state.replace(batch_stats=updates['batch_stats'])\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def accuracy(state, batch):\n",
    "  logits = state.apply_fn({\n",
    "    'params': state.params, \n",
    "    'batch_stats': state.batch_stats,\n",
    "  }, x=batch[0], train=False)\n",
    "  \n",
    "  preds = jnp.argmax(logits, axis=1)\n",
    "  acc = jnp.mean(preds == batch[1])\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 15:11:55.958293: W pjrt_plugin/src/mps_client.cc:534] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M3 Pro\n",
      "\n",
      "systemMemory: 18.00 GB\n",
      "maxCacheSize: 6.00 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flax_model = CNN()\n",
    "state = create_train_state(flax_model, jax.random.PRNGKey(0), learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train acc: 0.6185528635978699, test acc: 0.571093738079071\n",
      "[epoch 2] train acc: 0.7406662106513977, test acc: 0.677734375\n",
      "[epoch 3] train acc: 0.7697538137435913, test acc: 0.680468738079071\n",
      "[epoch 4] train acc: 0.9041579365730286, test acc: 0.770703136920929\n",
      "[epoch 5] train acc: 0.8265610337257385, test acc: 0.71875\n",
      "[epoch 6] train acc: 0.9856875538825989, test acc: 0.8402343988418579\n",
      "[epoch 7] train acc: 0.9886244535446167, test acc: 0.826953113079071\n",
      "[epoch 8] train acc: 0.9963389039039612, test acc: 0.8648437261581421\n",
      "[epoch 9] train acc: 0.9972541928291321, test acc: 0.85546875\n",
      "[epoch 10] train acc: 0.9988232254981995, test acc: 0.8675781488418579\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    state = train_step(state, batch)\n",
    "  \n",
    "  train_acc_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    acc = accuracy(state, batch)\n",
    "    train_acc_list.append(acc)\n",
    "\n",
    "  for batch in test_ds.as_numpy_iterator():\n",
    "    acc = accuracy(state, batch)\n",
    "    test_acc_list.append(acc)\n",
    "  \n",
    "  train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "  test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "  print(f'[epoch {epoch + 1}] train acc: {train_acc}, test acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_save_path = 'checkpoints/params.json'\n",
    "params_dict = {k1: {k2: v2.tolist() for k2, v2 in v1.items()} for k1, v1 in state.params.items()}\n",
    "with open(params_save_path, 'w') as f:\n",
    "  json.dump(params_dict, f)\n",
    "\n",
    "batch_stats_save_path = 'checkpoints/batch_stats.json'\n",
    "batch_stats_dict = {k1: {k2: v2.tolist() for k2, v2 in v1.items()} for k1, v1 in state.batch_stats.items()}\n",
    "with open(batch_stats_save_path, 'w') as f:\n",
    "  json.dump(batch_stats_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
