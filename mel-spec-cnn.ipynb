{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp, jit, value_and_grad, Array\n",
    "from jax.typing import ArrayLike\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.core import FrozenDict\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preproscessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('musicnet/musicnet_metadata.csv')\n",
    "data_files = glob('musicnet/musicnet/*/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 512\n",
    "\n",
    "def wav_to_mel_spec(path: str) -> np.ndarray:\n",
    "  y, sr = librosa.load(path)\n",
    "  spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "  return np.abs(librosa.amplitude_to_db(spec, ref=np.max))\n",
    "\n",
    "data = [wav_to_mel_spec(path) for path in data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = [int(path[-8:-4]) for path in data_files]\n",
    "labels = [metadata[metadata['id'] == i]['ensemble'].values[0] for i in data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_nums = {label: i for i, label in enumerate(sorted(set(labels)))}\n",
    "nums_to_labels = {i: label for label, i in labels_to_nums.items()}\n",
    "\n",
    "labels = [labels_to_nums[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "new_labels = []\n",
    "\n",
    "sample_len = 512\n",
    "\n",
    "for x, y in zip(data, labels):\n",
    "  for i in range(0, x.shape[1] - sample_len + 1, sample_len):\n",
    "    new_data.append(np.expand_dims(x[:, i:i + sample_len], axis=2))\n",
    "    new_labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.array(new_data, np.float32)\n",
    "y_full = np.array(new_labels, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_full, y_full, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "batch_size = 16\n",
    "train_ds = train_ds.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, x: ArrayLike, training: bool) -> Array:\n",
    "    x = (nn.Conv(features=8, kernel_size=(3, 3), use_bias=False))(x)\n",
    "    x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = nn.Conv(features=8, kernel_size=(3, 3), use_bias=False)(x)\n",
    "    x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = nn.Conv(features=8, kernel_size=(3, 3), use_bias=False)(x)\n",
    "    x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    \n",
    "    x = nn.Dense(features=128)(x)\n",
    "    x = nn.relu(x)\n",
    "\n",
    "    x = nn.Dense(features=64)(x)\n",
    "    x = nn.relu(x)\n",
    "\n",
    "    x = nn.Dense(features=21)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "  batch_stats: FrozenDict\n",
    "\n",
    "def create_train_state(model: CNN, rng_key: Array, learning_rate: float) -> TrainState:\n",
    "  variables = model.init(rng_key, x=jnp.ones((1, *x_train.shape[1:])), training=False)\n",
    "  return TrainState.create(\n",
    "    apply_fn=model.apply, \n",
    "    params=variables['params'],\n",
    "    batch_stats=variables['batch_stats'], \n",
    "    tx=optax.adamw(learning_rate, weight_decay=2e-3),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step(state: TrainState, batch: ArrayLike) -> TrainState:\n",
    "  \n",
    "  def loss_fn(params: FrozenDict) -> tuple[Array, FrozenDict]:\n",
    "    logits, updates = state.apply_fn({\n",
    "      'params': params, \n",
    "      'batch_stats': state.batch_stats,\n",
    "    }, x=batch[0], training=True, mutable=['batch_stats'])\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, updates\n",
    "  \n",
    "  grad_fn = value_and_grad(loss_fn, has_aux=True)\n",
    "  (_, updates), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  state = state.replace(batch_stats=updates['batch_stats'])\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def accuracy(state: TrainState, batch: ArrayLike):\n",
    "  logits = state.apply_fn({\n",
    "    'params': state.params, \n",
    "    'batch_stats': state.batch_stats,\n",
    "  }, x=batch[0], training=False)\n",
    "  \n",
    "  preds = jnp.argmax(logits, axis=1)\n",
    "  acc = jnp.mean(preds == batch[1])\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 20:37:12.821952: W pjrt_plugin/src/mps_client.cc:534] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M3 Pro\n",
      "\n",
      "systemMemory: 18.00 GB\n",
      "maxCacheSize: 6.00 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flax_model = CNN()\n",
    "state = create_train_state(flax_model, jax.random.PRNGKey(0), learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train acc: 0.7633345127105713, test acc: 0.7220911979675293\n",
      "[epoch 2] train acc: 0.7240667343139648, test acc: 0.6623427867889404\n",
      "[epoch 3] train acc: 0.8015642762184143, test acc: 0.7197327017784119\n",
      "[epoch 4] train acc: 0.9355345964431763, test acc: 0.8132861852645874\n",
      "[epoch 5] train acc: 0.9849318861961365, test acc: 0.8419811129570007\n",
      "[epoch 6] train acc: 0.990435004234314, test acc: 0.8525943160057068\n",
      "[epoch 7] train acc: 0.9962002038955688, test acc: 0.8620283007621765\n",
      "[epoch 8] train acc: 0.9854559898376465, test acc: 0.8392295837402344\n",
      "[epoch 9] train acc: 0.99947589635849, test acc: 0.8820754885673523\n",
      "[epoch 10] train acc: 0.9993448853492737, test acc: 0.8651729822158813\n",
      "[epoch 11] train acc: 0.9997379183769226, test acc: 0.8860062956809998\n",
      "[epoch 12] train acc: 0.9998689889907837, test acc: 0.885613203048706\n",
      "[epoch 13] train acc: 0.9998689889907837, test acc: 0.8706761002540588\n",
      "[epoch 14] train acc: 0.9975104928016663, test acc: 0.8694968819618225\n",
      "[epoch 15] train acc: 1.0, test acc: 0.8844339847564697\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    state = train_step(state, batch)\n",
    "  \n",
    "  train_acc_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    acc = accuracy(state, batch)\n",
    "    train_acc_list.append(acc)\n",
    "\n",
    "  for batch in test_ds.as_numpy_iterator():\n",
    "    acc = accuracy(state, batch)\n",
    "    test_acc_list.append(acc)\n",
    "  \n",
    "  train_acc = sum(train_acc_list) / len(train_acc_list)\n",
    "  test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "\n",
    "  print(f'[epoch {epoch + 1}] train acc: {train_acc}, test acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_save_path = 'checkpoints/params.json'\n",
    "params_dict = {k1: {k2: v2.tolist() for k2, v2 in v1.items()} for k1, v1 in state.params.items()}\n",
    "with open(params_save_path, 'w') as f:\n",
    "  json.dump(params_dict, f)\n",
    "\n",
    "batch_stats_save_path = 'checkpoints/batch_stats.json'\n",
    "batch_stats_dict = {k1: {k2: v2.tolist() for k2, v2 in v1.items()} for k1, v1 in state.batch_stats.items()}\n",
    "with open(batch_stats_save_path, 'w') as f:\n",
    "  json.dump(batch_stats_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
